{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pragnya08/Sentiment-Classification-for-Social-Media/blob/main/sentiment_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9z_rTMeXYLe"
      },
      "source": [
        "#### Import necessary packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmFDelDlXYLf"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import re\n",
        "import os\n",
        "from os.path import join\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import nltk\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv6hAJfhXYLf"
      },
      "outputs": [],
      "source": [
        "training_dataset = '/content/semeval-tweets/twitter-training-data.txt'\n",
        "dev_dataset = '/content/semeval-tweets/twitter-dev-data.txt'\n",
        "glove_path = '/content/glove.6B.100d.txt'\n",
        "# Define test sets\n",
        "testsets = ['/content/semeval-tweets/twitter-test1.txt', '/content/semeval-tweets/twitter-test2.txt', '/content/semeval-tweets/twitter-test3.txt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6LBi8AmXYLg"
      },
      "outputs": [],
      "source": [
        "# Skeleton: Evaluation code for the test sets\n",
        "def read_test(testset):\n",
        "    '''\n",
        "    readin the testset and return a dictionary\n",
        "    :param testset: str, the file name of the testset to compare\n",
        "    '''\n",
        "    id_gts = {}\n",
        "    with open(testset, 'r', encoding='utf8') as fh:\n",
        "        for line in fh:\n",
        "            fields = line.split('\\t')\n",
        "            tweetid = fields[0]\n",
        "            gt = fields[1]\n",
        "\n",
        "            id_gts[tweetid] = gt\n",
        "\n",
        "    return id_gts\n",
        "\n",
        "\n",
        "def confusion(id_preds, testset, classifier):\n",
        "    '''\n",
        "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
        "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
        "    :param testset: str, the file name of the testset to compare\n",
        "    :classifier: str, the name of the classifier\n",
        "    '''\n",
        "    id_gts = read_test(testset)\n",
        "\n",
        "    gts = []\n",
        "    for m, c1 in id_gts.items():\n",
        "        if c1 not in gts:\n",
        "            gts.append(c1)\n",
        "\n",
        "    gts = ['positive', 'negative', 'neutral']\n",
        "\n",
        "    conf = {}\n",
        "    for c1 in gts:\n",
        "        conf[c1] = {}\n",
        "        for c2 in gts:\n",
        "            conf[c1][c2] = 0\n",
        "\n",
        "    for tweetid, gt in id_gts.items():\n",
        "        if tweetid in id_preds:\n",
        "            pred = id_preds[tweetid]\n",
        "        else:\n",
        "            pred = 'neutral'\n",
        "        conf[pred][gt] += 1\n",
        "\n",
        "    print(''.ljust(12) + '  '.join(gts))\n",
        "\n",
        "    for c1 in gts:\n",
        "        print(c1.ljust(12), end='')\n",
        "        for c2 in gts:\n",
        "            if sum(conf[c1].values()) > 0:\n",
        "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
        "            else:\n",
        "                print('0.000     ', end='')\n",
        "        print('')\n",
        "\n",
        "    print('')\n",
        "\n",
        "\n",
        "def evaluate(id_preds, testset, classifier):\n",
        "    '''\n",
        "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
        "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
        "    :param testset: str, the file name of the testset to compare\n",
        "    :classifier: str, the name of the classifier\n",
        "    '''\n",
        "    id_gts = read_test(testset)\n",
        "\n",
        "    acc_by_class = {}\n",
        "    for gt in ['positive', 'negative', 'neutral']:\n",
        "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
        "\n",
        "    catf1s = {}\n",
        "\n",
        "    ok = 0\n",
        "    for tweetid, gt in id_gts.items():\n",
        "        if tweetid in id_preds:\n",
        "            pred = id_preds[tweetid]\n",
        "        else:\n",
        "            pred = 'neutral'\n",
        "\n",
        "        if gt == pred:\n",
        "            ok += 1\n",
        "            acc_by_class[gt]['tp'] += 1\n",
        "        else:\n",
        "            acc_by_class[gt]['fn'] += 1\n",
        "            acc_by_class[pred]['fp'] += 1\n",
        "\n",
        "    catcount = 0\n",
        "    itemcount = 0\n",
        "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
        "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
        "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
        "\n",
        "    microtp = 0\n",
        "    microfp = 0\n",
        "    microtn = 0\n",
        "    microfn = 0\n",
        "    for cat, acc in acc_by_class.items():\n",
        "        catcount += 1\n",
        "\n",
        "        microtp += acc['tp']\n",
        "        microfp += acc['fp']\n",
        "        microtn += acc['tn']\n",
        "        microfn += acc['fn']\n",
        "\n",
        "        p = 0\n",
        "        if (acc['tp'] + acc['fp']) > 0:\n",
        "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
        "\n",
        "        r = 0\n",
        "        if (acc['tp'] + acc['fn']) > 0:\n",
        "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
        "\n",
        "        f1 = 0\n",
        "        if (p + r) > 0:\n",
        "            f1 = 2 * p * r / (p + r)\n",
        "\n",
        "        catf1s[cat] = f1\n",
        "\n",
        "        n = acc['tp'] + acc['fn']\n",
        "\n",
        "        macro['p'] += p\n",
        "        macro['r'] += r\n",
        "        macro['f1'] += f1\n",
        "\n",
        "        if cat in ['positive', 'negative']:\n",
        "            semevalmacro['p'] += p\n",
        "            semevalmacro['r'] += r\n",
        "            semevalmacro['f1'] += f1\n",
        "\n",
        "        itemcount += n\n",
        "\n",
        "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
        "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
        "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
        "\n",
        "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
        "\n",
        "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs8MopTJZcKs"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pZ-3JMVZZ2d",
        "outputId": "84080432-32bb-463d-8dbd-97c821795c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_tweet(tweet):\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
        "    tweet = re.sub(r'\\@\\w+|\\#', '', tweet)\n",
        "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
        "    tweet = re.sub(r'\\d+', '', tweet)  # Remove digits\n",
        "    tweet = tweet.lower()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(tweet)\n",
        "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tweet = [lemmatizer.lemmatize(w) for w in filtered_tweet]\n",
        "    return ' '.join(lemmatized_tweet)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iSHvuE5XYLh"
      },
      "source": [
        "#### Load training set, dev set and testing set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7ek_wLyXYLh"
      },
      "outputs": [],
      "source": [
        "# Load training set, dev set and testing set\n",
        "data = {}\n",
        "tweetids = {}\n",
        "tweetgts = {}\n",
        "tweets = {}\n",
        "\n",
        "for dataset in [training_dataset] + testsets:\n",
        "    dataset_key = dataset.split('/')[-1]\n",
        "    with open(dataset, 'r', encoding='utf8') as sentiment:\n",
        "          if dataset_key not in data:\n",
        "              data[dataset_key] = []\n",
        "              tweets[dataset_key]= []\n",
        "              tweetids[dataset_key] = []\n",
        "              tweetgts[dataset_key] = []\n",
        "          for line in sentiment:\n",
        "              tweet_id, sentiment, tweet_text = line.strip().split('\\t')\n",
        "              preprocessed_text = preprocess_tweet(tweet_text)\n",
        "              data[dataset_key].append((tweet_id, sentiment, preprocessed_text))\n",
        "              tweets[dataset_key].append(preprocessed_text)\n",
        "              tweetids[dataset_key].append(tweet_id)\n",
        "              tweetgts[dataset_key].append(sentiment)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zXYicdQfRZI"
      },
      "source": [
        "## Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhjxvDhKfQtO"
      },
      "outputs": [],
      "source": [
        "# GloVe embeddings loader\n",
        "def load_glove_embeddings(path):\n",
        "    embeddings_dict = {}\n",
        "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], \"float32\")\n",
        "            embeddings_dict[word] = vector\n",
        "    return embeddings_dict\n",
        "\n",
        "def tweet_to_embedding(tweet, embeddings_dict, embedding_dim=100):\n",
        "    words = tweet.split()\n",
        "    accumulated_vector = np.zeros(embedding_dim)\n",
        "    word_count = 0\n",
        "    for word in words:\n",
        "        if word in embeddings_dict:\n",
        "            accumulated_vector += embeddings_dict[word]\n",
        "            word_count += 1\n",
        "    return accumulated_vector / word_count if word_count > 0 else accumulated_vector\n",
        "\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "glove_data = {}\n",
        "for dataset_key in tweets.keys():\n",
        "    glove_data[dataset_key] = np.array([tweet_to_embedding(tweet, glove_embeddings, 100) for tweet in tweets[dataset_key]])\n",
        "\n",
        "\n",
        "\n",
        "#Bow\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X_bow_training = vectorizer.fit_transform(tweets['twitter-training-data.txt'])\n",
        "X_train_glove = glove_data['twitter-training-data.txt']\n",
        "y_train = np.array(tweetgts['twitter-training-data.txt'])\n",
        "\n",
        "\n",
        "# Function to preprocess text and extract features for a given dataset\n",
        "def preprocess_and_extract_features(dataset_path, vectorizer, glove_embeddings):\n",
        "    # Initialize lists to hold tweets and their IDs\n",
        "    tweets = []\n",
        "    tweet_ids = []  # List to store tweet IDs\n",
        "\n",
        "    with open(dataset_path, 'r', encoding='utf8') as file:\n",
        "        for line in file:\n",
        "            tweet_id, sentiment, tweet_text = line.strip().split('\\t')\n",
        "            preprocessed_text = preprocess_tweet(tweet_text)\n",
        "            tweets.append(preprocessed_text)\n",
        "            tweet_ids.append(tweet_id)  # Append the tweet ID\n",
        "\n",
        "    # Extract BoW features\n",
        "    X_bow = vectorizer.transform(tweets).toarray()  # Convert to dense array\n",
        "\n",
        "    # Extract GloVe features\n",
        "    X_glove = np.array([tweet_to_embedding(tweet, glove_embeddings) for tweet in tweets])\n",
        "\n",
        "    return tweet_ids, X_bow, X_glove\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjAD5ILcIoRm",
        "outputId": "97a430a9-d531-4a69-ebe7-cf8ab7b6d260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "    def forward(self, x):\n",
        "      lstm_out, _ = self.lstm(x)\n",
        "      output = self.fc(lstm_out)\n",
        "      return output\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MvMtHpsXYLh"
      },
      "source": [
        "#### Build sentiment classifiers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jb2_oWXHXYLh",
        "outputId": "bb01f7a9-6a31-4222-c1a9-ade7958e246c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded .....'svm_bow_model.pkl'\n",
            "/content/semeval-tweets/twitter-test1.txt (bow-svm): 0.557\n",
            "/content/semeval-tweets/twitter-test2.txt (bow-svm): 0.573\n",
            "/content/semeval-tweets/twitter-test3.txt (bow-svm): 0.526\n",
            "Model loaded .....'svm_glove_model.pkl'\n",
            "/content/semeval-tweets/twitter-test1.txt (glove-svm): 0.390\n",
            "/content/semeval-tweets/twitter-test2.txt (glove-svm): 0.427\n",
            "/content/semeval-tweets/twitter-test3.txt (glove-svm): 0.411\n",
            "Model loaded .....'logistic_regression_bow_model.pkl'\n",
            "/content/semeval-tweets/twitter-test1.txt (bow-logistic_regression): 0.552\n",
            "/content/semeval-tweets/twitter-test2.txt (bow-logistic_regression): 0.576\n",
            "/content/semeval-tweets/twitter-test3.txt (bow-logistic_regression): 0.536\n",
            "Model loaded .....'logistic_regression_glove_model.pkl'\n",
            "/content/semeval-tweets/twitter-test1.txt (glove-logistic_regression): 0.435\n",
            "/content/semeval-tweets/twitter-test2.txt (glove-logistic_regression): 0.444\n",
            "/content/semeval-tweets/twitter-test3.txt (glove-logistic_regression): 0.447\n",
            "Model loaded .....'LSTM_glove_model.pkl'\n",
            "/content/semeval-tweets/twitter-test1.txt (glove-LSTM): 0.468\n",
            "/content/semeval-tweets/twitter-test2.txt (glove-LSTM): 0.465\n",
            "/content/semeval-tweets/twitter-test3.txt (glove-LSTM): 0.507\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for classifier in ['svm','logistic_regression','LSTM']:\n",
        "    for features in ['bow', 'glove']:\n",
        "\n",
        "\n",
        "        if features == 'bow':\n",
        "            X_train = X_bow_training\n",
        "        elif features == 'glove':\n",
        "            X_train = X_train_glove\n",
        "        # Skeleton: Creation and training of the classifiers\n",
        "        if classifier == 'svm':\n",
        "            model_filename = classifier + '_' + features + '_model.pkl'\n",
        "            if os.path.exists(model_filename):\n",
        "              with open(model_filename, 'rb') as f:\n",
        "                model = pickle.load(f)\n",
        "              print(f\"Model loaded .....'{model_filename}'\")\n",
        "            else:\n",
        "              model = SVC(kernel='linear',C=1.0)\n",
        "              model.fit(X_train, y_train)\n",
        "              print('Training ' + classifier)\n",
        "              with open(model_filename, 'wb') as f:\n",
        "                pickle.dump(model, f)\n",
        "              print(f'Model saved as {model_filename}')\n",
        "        elif classifier == 'logistic_regression':\n",
        "            model_filename = classifier + '_' + features + '_model.pkl'\n",
        "            if os.path.exists(model_filename):\n",
        "              with open(model_filename, 'rb') as f:\n",
        "                model = pickle.load(f)\n",
        "              print(f\"Model loaded .....'{model_filename}'\")\n",
        "            else:\n",
        "              model = LogisticRegression(max_iter=1000)\n",
        "              model.fit(X_train, y_train)\n",
        "              print('Training ' + classifier)\n",
        "              with open(model_filename, 'wb') as f:\n",
        "                pickle.dump(model, f)\n",
        "              print(f'Model saved as {model_filename}')\n",
        "        elif classifier == 'LSTM':\n",
        "            # write the LSTM classifier here\n",
        "            if features == 'bow':\n",
        "                continue\n",
        "            label_encoder = LabelEncoder()\n",
        "            y_train_encoded = label_encoder.fit_transform(np.array(tweetgts['twitter-training-data.txt']))\n",
        "            model_filename = classifier + '_' + features + '_model.pkl'\n",
        "            if os.path.exists(model_filename):\n",
        "                with open(model_filename, 'rb') as f:\n",
        "                  model = pickle.load(f)\n",
        "                print(f\"Model loaded .....'{model_filename}'\")\n",
        "            else:\n",
        "              print('Training ' + classifier)\n",
        "              # Directly use LabelEncoder on your labels\n",
        "              label_encoder = LabelEncoder()\n",
        "              y_train_encoded = label_encoder.fit_transform(np.array(tweetgts['twitter-training-data.txt']))\n",
        "\n",
        "              # Convert labels to tensor\n",
        "              y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "\n",
        "              # Convert GloVe features to tensor\n",
        "              X_train_glove_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "\n",
        "              # Create dataset and dataloader for GloVe\n",
        "              train_glove_dataset = TensorDataset(X_train_glove_tensor, y_train_tensor)\n",
        "              train_glove_loader = DataLoader(train_glove_dataset, batch_size=64, shuffle=True,drop_last=True)\n",
        "\n",
        "              # Define LSTM model\n",
        "              input_dim = 100  # Assuming you're using GloVe vectors of 100 dimensions\n",
        "              hidden_dim = 256\n",
        "              output_dim = len(label_encoder.classes_)\n",
        "              model = LSTMModel(input_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "              # Loss and optimizer\n",
        "              loss_function = nn.CrossEntropyLoss()\n",
        "              optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "              # Train LSTM\n",
        "              epochs = 5\n",
        "              for epoch in range(epochs):\n",
        "                model.train()\n",
        "                for inputs, labels in train_glove_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                    outputs = model(inputs)\n",
        "\n",
        "                    # Dynamically calculate loss based on the actual size of the batch\n",
        "                    loss = loss_function(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "              with open(model_filename, 'wb') as f:\n",
        "                pickle.dump(model,f)\n",
        "              print(f'Model saved as {model_filename}')\n",
        "        else:\n",
        "          print('Unknown classifier name' + classifier)\n",
        "          continue\n",
        "\n",
        "        # Predition performance of the classifiers\n",
        "        for testset in testsets:\n",
        "            tweet_ids, X_bow_test, X_glove_test = preprocess_and_extract_features(testset, vectorizer, glove_embeddings)\n",
        "\n",
        "\n",
        "            X_test = X_bow_test if features == 'bow' else X_glove_test\n",
        "\n",
        "            if classifier == 'LSTM':\n",
        "                X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(X_test_tensor)\n",
        "                    _, predictions_numeric = torch.max(outputs, 1)\n",
        "                    # Convert numeric predictions back to original labels\n",
        "                    predictions_labels = label_encoder.inverse_transform(predictions_numeric.cpu().numpy())\n",
        "                # Map tweet IDs to their predicted labels for evaluation\n",
        "                id_preds = {tweet_id: pred_label for tweet_id, pred_label in zip(tweet_ids, predictions_labels)}\n",
        "            else:\n",
        "               predictions = model.predict(X_test)\n",
        "               id_preds = {tweet_id: pred for tweet_id, pred in zip(tweet_ids, predictions)}\n",
        "\n",
        "            testset_name = testset\n",
        "            testset_path = join('semeval-tweets', testset_name)\n",
        "            evaluate(id_preds, testset_path, features + '-' + classifier)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}